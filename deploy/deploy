#!/bin/sh
# exit if any command fails
set -e

chart_version=${CHART_VERSION:-0.2.3}
helm_version=3.0.3
chart_ref=cf-review-env
short_name=$(echo -n $NAME | cut -c1-15)
hash=$(echo "$NAME" | rhash -p "%c" -)
namespace=re-$short_name-$hash

config_context() {
  kubectl config use-context "$KUBE_CONTEXT"
}

namespace_apply() {
  # Variables need to be export in order to be replaced by envsubst
  updated_at=$(date --utc +%s)
  export UPDATED_AT=$updated_at
  export NAMESPACE=$namespace
  cat <<EOF | envsubst | kubectl apply -f -
apiVersion: v1
kind: Namespace
metadata:
  labels:
    app.kubernetes.io/instance: $NAMESPACE
    app.kubernetes.io/name: cf-review-env
    app.kubernetes.io/pull_request_number: "$CF_PULL_REQUEST_NUMBER"
    app.kubernetes.io/repository_name: "$CF_REPO_NAME"
    app.kubernetes.io/repository_owner: "$CF_REPO_OWNER"
    app.kubernetes.io/updated_at: "$UPDATED_AT"
  name: $NAMESPACE
EOF
}

apply_secret() {
  # If the user doesn't have secrets we need to generate an empty file
  # to make our chart happy, without this the deployment breaks
  if [ ! -f "/codefresh/volume/secrets.env" ]; then
    touch /codefresh/volume/secrets.env
  fi

  kubectl create secret generic "$namespace" --from-env-file=/codefresh/volume/secrets.env \
    --dry-run=client -o yaml --save-config=true --namespace "$namespace" | kubectl apply -f -
}

generate_pull_secret() {
  codefresh generate image-pull-secret --cluster "$KUBE_CONTEXT" --namespace "$namespace" \
    --registry "$APP_REGISTRY_NAME"
}

install_chart() {
  # We need to use env here because of the CUSTOM vars
  # they have special charectes that export can't handle
  env \
  ACTION=install \
  CHART_REPO_URL="$HELM_REPO_URL" \
  CHART_REF=$chart_ref \
  KUBE_CONTEXT="$KUBE_CONTEXT" \
  RELEASE_NAME="$namespace" \
  CHART_VERSION="$chart_version" \
  HELM_VERSION=$helm_version \
  CUSTOM_image_tag="$APP_IMAGE_TAG" \
  CUSTOM_image_repository="$APP_IMAGE_URL" \
  CUSTOM_imagePullSecrets="$(kubectl get secret --namespace $namespace --field-selector='type=kubernetes.io/dockercfg' -o=jsonpath='{.items[0].metadata.name}')" \
  CUSTOM_envFrom_secretRef_name="$namespace" \
  CUSTOM_"ingress_hosts[0]_host=$namespace.$APP_DOMAIN" \
  CUSTOM_"ingress_hosts[0]_paths[0]=/" \
  CUSTOM_"ingress_tls[0]_hosts[0]=$namespace.$APP_DOMAIN" \
  CUSTOMFILE_0="$VALUES_FILE" \
  SKIP_CF_STABLE_HELM_REPO=true \
  WAIT=true \
  /opt/bin/release_chart
}

test_chart() {
  TEST_CONNECTION=${TEST_CONNECTION:-true}
  if [ "$TEST_CONNECTION" = true ];
  then
    env ACTION=auth \
    SKIP_CF_STABLE_HELM_REPO=true \
    HELM_VERSION=$helm_version \
    CHART_VERSION="$chart_version" \
    KUBE_CONTEXT="$KUBE_CONTEXT" \
    /opt/bin/release_chart
    helm test "$namespace" --namespace "$namespace"

    # It is necessary to remove the test pod from the helm test bacause HPA metrics may
    # fail when we have a pod with completed status.
    # Issue: https://github.com/kubernetes/kubernetes/issues/79365
    # Issues: https://github.com/kubernetes/kubernetes/issues?page=1&q=is%3Aissue+is%3Aopen+HPA
    echo "INFO: Removed the completed Test Pod.";
    kubectl delete pod --selector="app.kubernetes.io/managed-by=Helm" --namespace "$namespace"
  else
    echo "Connection test is disabled.";
  fi
}

export_variables() {
  # It is necessary to export these variables because we are using it on the pipelines
  cf_export NAMESPACE="$namespace"
  cf_export URL="https://$namespace.$APP_DOMAIN"
  cf_export K8S_CONTEXT_NAME="$KUBE_CONTEXT"
}

config_context
namespace_apply
apply_secret
generate_pull_secret
install_chart
test_chart
export_variables
